---
layout: post
title: "个人版 AI 辅助系统的尝试"
description: "The Old Dwarf Forged A New Toy In His Mountain Workshop"
category: 
tags: ["AI", "Python", "Java", "Parsec"]
---

在 CSDN  的时候，我就一直想要有自己的 AI 工作环境。我们组只有一台高配的办公服务器，用于训练模型，分析数据。通常来说这台机器都很忙。如果想要
做一些研究工作或试验，资源就有点紧张了。而我自己的工作机，虽然是一台买了只有四五年的高配 MacBook Pro，但是做 AI 已经不太够用。即使训练一个非常
朴素的文本分类器，也要花费太多时间。那几年我偏好用传统的算法而非 AI 工具解决问题，其实也有这个原因。并非我不能驾驭 AI 技术，只是因地制宜而已。
何况团队里的年轻人们对 AI 相关工作做的非常好，我更关注那些更需要有人站出来解决的东西。

说起来还是要感谢 CSDN，我正是用离职时的补偿，买了这台满配的 MacBook Pro 。128 内存，
16 core cpu + 40 core gpu + 16 core npu 的 M3 MAX。足够我运行常规的 AI 算法，一些规模不太大的 LLM 模型也完全可以运行。

对于 AI 系统，我的目标是：

1. 虽然现在互联网上有越来越多的 AI 服务，免费的也不在少数，但是我仍然希望可以建立一套可以脱离外部服务的私人工具系统
2. 这套 AI 系统首先应该对我有实用价值，能够完成一些常规的软件应用不容易做到的事情。比如一些智能化的文档生成、翻译和代码生成工作
3. 我可以通过调整和训练，使AI 系统更符合自己的需求，这里面包含了应用软件配置、开发，模型的调整甚至训练，总之，这套系统对我应该是个白盒。

经过一段时间的尝试，我初步的达到了这些目的。

最初，我尝试过直接用 torch 或 tensorflow 这样的框架运行模型，后来很快发现即使这台满配的机器，运行 3B 的 LLM 也有些吃力，毕竟现在的 AI 
产业已经普遍使用价格高昂的超级显卡。而我，只是一个希望一次投资可以尽可能多用的穷人。

很快，我开始尝试用同好们推荐的 llama.cpp  和 ollama 来运行模型。尽管看起来这两个工具有些重叠，它们都可以基于 cpp 构建的运行时运行模型，
都提供了 server ，部署客户端也很方便。但是经过实践，llama.cpp 有更好的泛用性，有些模型 ollama 不能识别，但是 llama.cpp 可以加载并处理
为 gguf 格式。而经过 llama.cpp 处理后的模型，ollama 是可以加载的。另一方面，在我的机器上，llama.cpp 运行 70b 的模型，慢到无法使用，
而 ollama 可以。至少在 M3 硬件环境中，ollama 表现出了更好的性能。

再接下来，就是我在 [oliva](https://github.com/MarchLiu/oliva) 项目中发布的词法分析器工具，它可以把几个我常用的编程语言，C、Java、
Python、Scala 等等处理为 alpaca lora 格式。用于微调模型，目前我正在尝试基于 llamacode 的几个小规模版本，训练出一个适合我自己的版本。
因为从我自己的体验看，codellama 对c语言和 scala 的支持并不强，高版本的 java 应该也还没有引入，而这些是我需要的。甚至可以说我搭建这套
私有 AI 系统，一个重要的目标就是辅助我写一些 c 代码。目前的实验来看，[llama factory](https://github.com/hiyouga/LLaMA-Factory) 
项目能够完成这个工作。

我完全没想到的是，整个工具链上，我最不满意的居然是客户端，是的，我试用了 ollama 官网上推荐的好几个客户端应用，都不符合我的期待，有些有配
置问题，有些对中文支持的非常差，有些使用起来很繁琐。于是我用 Python 写了一个命令行工具 
[Blue Shell](https://pypi.org/project/blueshell/)。用于日常工作。这个工具支持行编辑，能够可靠的支持中文，可以支持 codellama 的
markdown 输出，可以方便的连接指定的 ollama 。也许将来，我还会再开发一些 GUI 客户端或者开发工具的插件。但是目前，这个工具体系已经初步的
运行起来了。我已经开始用 AI 为 Jaskell 项目生成单元测试代码——嗯其实我也希望它能做一些更智能的编程工作，但是目前看，真正需要创造力的部分，
还是不能指望这些概率模型的。